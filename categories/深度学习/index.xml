<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>深度学习 on 想起床的方差</title>
        <link>https://wakingupdelta.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in 深度学习 on 想起床的方差</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>想起床的方差</copyright>
        <lastBuildDate>Sun, 12 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://wakingupdelta.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>注意力机制概览</title>
        <link>https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/</link>
        <pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate>
        
        <guid>https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/</guid>
        <description>&lt;h1 id=&#34;一背景&#34;&gt;一、背景
&lt;/h1&gt;&lt;p&gt;注意力机制可有效提高模型对特征的提取能力，进而提高模型精度。本文首先对经典的三种注意力学习方法进行介绍，后结合自己一点点浅薄认知，对注意力机制后续的改进方向进行归纳，希望对大家能有帮助&lt;/p&gt;
&lt;h1 id=&#34;二前置知识&#34;&gt;二、前置知识
&lt;/h1&gt;&lt;h2 id=&#34;21-通道注意力&#34;&gt;2.1 通道注意力
&lt;/h2&gt;&lt;img src=&#34;image-20240903143439161.png&#34; alt=&#34;image-20240903143439161&#34; style=&#34;zoom:67%;&#34; /&gt;
对特征图进行全局平均池化，后经过两层全连接层，最后使用Sigmoid输出0-1之间的权重
&lt;h2 id=&#34;22-空间注意力&#34;&gt;2.2 空间注意力
&lt;/h2&gt;&lt;img src=&#34;image-20240903143451755.png&#34; alt=&#34;image-20240903143451755&#34; style=&#34;zoom:67%;&#34; /&gt;
将特征图进行压缩，得到空间方向上的权重图
&lt;h2 id=&#34;23-self-attention&#34;&gt;2.3 self-attention
&lt;/h2&gt;&lt;img src=&#34;image-20240903143502737.png&#34; alt=&#34;image-20240903143502737&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;这里借助了Non-local Neural Networks中的一张图。self-attention本质上是一种空间注意机制，如上图所示，需要求解一个[THW, THW]的矩阵，该矩阵中的每一行，表示原特征图中的一点与其余点的关系&lt;/p&gt;
&lt;h1 id=&#34;三各种改进版本&#34;&gt;三、各种改进版本
&lt;/h1&gt;&lt;p&gt;将改进的注意力机制主要分为四类&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;跨特征：构建不同的特征表达，后进行融合&lt;/li&gt;
&lt;li&gt;分组：对特征图按通道分组后，再使用注意力机制&lt;/li&gt;
&lt;li&gt;self-attention：本质上，self-attention就是一系列矩阵运算，对运算过程中的某些步骤进行优化&lt;/li&gt;
&lt;li&gt;空间+通道：很自然的想法，同时使用两种注意力机制&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;31-跨特征&#34;&gt;3.1 跨特征
&lt;/h2&gt;&lt;h4 id=&#34;311-bisenet&#34;&gt;3.1.1 BiSeNet
&lt;/h4&gt;&lt;img src=&#34;image-20240903143515154.png&#34; alt=&#34;image-20240903143515154&#34; style=&#34;zoom:67%;&#34; /&gt;
对不同层次的特征使用通道注意力机制进行融合。这应该属于应用创新，没有构建新的注意力机制，而是将老的注意力机制应用到特征融合方向
&lt;h4 id=&#34;312-sknet&#34;&gt;3.1.2 SKNet
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143528059.png&#34;
	width=&#34;1257&#34;
	height=&#34;359&#34;
	srcset=&#34;https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143528059_hu4188395624d6c6de293fd145c8a93efd_185865_480x0_resize_box_3.png 480w, https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143528059_hu4188395624d6c6de293fd145c8a93efd_185865_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20240903143528059&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;350&#34;
		data-flex-basis=&#34;840px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;使用不同感受野卷积核构建多分支特征，将这些特征 add 后计算不同分支的同道注意力系数，后沿着通道方向进行加权求和&lt;/p&gt;
&lt;h2 id=&#34;32-分组&#34;&gt;3.2 分组
&lt;/h2&gt;&lt;h3 id=&#34;321-sgnet&#34;&gt;3.2.1 SGNet
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143537682.png&#34;
	width=&#34;1252&#34;
	height=&#34;426&#34;
	srcset=&#34;https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143537682_hud065ca2fd1dda1682e5e65c580ff9f53_319318_480x0_resize_box_3.png 480w, https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143537682_hud065ca2fd1dda1682e5e65c580ff9f53_319318_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20240903143537682&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;293&#34;
		data-flex-basis=&#34;705px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对每个组，先计算向量x的平均值（全局平均池化），后将利用该向量对特征图沿着通道 方向加权求和得到 空间注意力，再归一化，后经sigmoid函数输出&lt;/p&gt;
&lt;h3 id=&#34;322-sanet&#34;&gt;3.2.2 SANet
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143548571.png&#34;
	width=&#34;1256&#34;
	height=&#34;347&#34;
	srcset=&#34;https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143548571_hube3e2276944c2386941e589284b66220_149550_480x0_resize_box_3.png 480w, https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143548571_hube3e2276944c2386941e589284b66220_149550_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20240903143548571&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;361&#34;
		data-flex-basis=&#34;868px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;将特征沿着通道分组，对每一组，再分成 空间注意力 和 通道注意力。对空间注意力，使用Group normalization实现。将分组结果进行拼接，后使用类似ShufflfleNet v2的方法进行channel shuffle&lt;/p&gt;
&lt;h2 id=&#34;33-self-attention&#34;&gt;3.3 Self Attention
&lt;/h2&gt;&lt;h3 id=&#34;331-non-local-attention&#34;&gt;3.3.1 non local attention
&lt;/h3&gt;&lt;img src=&#34;image-20240903143603023.png&#34; alt=&#34;image-20240903143603023&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;定义了不同形式计算向量相似性的函数，提出self attention是non local attention 的特例&lt;/p&gt;
&lt;h3 id=&#34;332-gcnet&#34;&gt;3.3.2 GCNet
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143615249.png&#34;
	width=&#34;1260&#34;
	height=&#34;424&#34;
	srcset=&#34;https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143615249_hu1aa83e91a93954a70623cd5a573c4860_237508_480x0_resize_box_3.png 480w, https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143615249_hu1aa83e91a93954a70623cd5a573c4860_237508_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20240903143615249&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;297&#34;
		data-flex-basis=&#34;713px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;指出non local attention不同位置的attention map基本一致，只计算Cx1x1的特征图&lt;/p&gt;
&lt;h3 id=&#34;333-dual-attention&#34;&gt;3.3.3 dual attention
&lt;/h3&gt;&lt;img src=&#34;image-20240903143636442.png&#34; alt=&#34;image-20240903143636442&#34; style=&#34;zoom: 50%;&#34; /&gt;
&lt;p&gt;并联non local attention 和 GCnet&lt;/p&gt;
&lt;h2 id=&#34;34-空间通道&#34;&gt;3.4 空间+通道
&lt;/h2&gt;&lt;h3 id=&#34;341-bam&#34;&gt;3.4.1 BAM
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143651005.png&#34;
	width=&#34;1259&#34;
	height=&#34;445&#34;
	srcset=&#34;https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143651005_hu64b85001a552aa50dce13d8f55530e40_190059_480x0_resize_box_3.png 480w, https://wakingupdelta.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A6%82%E8%A7%88/image-20240903143651005_hu64b85001a552aa50dce13d8f55530e40_190059_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20240903143651005&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;282&#34;
		data-flex-basis=&#34;679px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;并联，attention map先相加（利用了广播机制），后与原特征图相乘&lt;/p&gt;
&lt;h3 id=&#34;342-scse&#34;&gt;3.4.2 scSE
&lt;/h3&gt;&lt;img src=&#34;image-20240903143703961.png&#34; alt=&#34;image-20240903143703961&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;并联，attention map先各自相乘，后取最大值得到最终输出&lt;/p&gt;
&lt;h1 id=&#34;参考文献&#34;&gt;参考文献
&lt;/h1&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/papers/Changqian_Yu_BiSeNet_Bilateral_Segmentation_ECCV_2018_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[1] BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[2] Selective Kernel Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1905.09646.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[3] Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2102.00240.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[4] SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[5] Non-local Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[6] GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[7] Dual Attention Network for Scene Segmentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1807.06514.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[8] BAM: Bottleneck Attention Module&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1808.08127.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[9] Recalibrating Fully Convolutional Networks with Spatial and Channel ‘Squeeze &amp;amp; Excitation’ Blocks&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>基于一致性的半监督学习</title>
        <link>https://wakingupdelta.github.io/p/%E5%9F%BA%E4%BA%8E%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</link>
        <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
        
        <guid>https://wakingupdelta.github.io/p/%E5%9F%BA%E4%BA%8E%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</guid>
        <description>&lt;h1 id=&#34;一背景&#34;&gt;一、背景
&lt;/h1&gt;&lt;p&gt;医学图像的标注成本十分高昂，其标注数据规模往往较小。通过半监督学习的方法，利用大量的未标注数据，可有效提高模型的学习能力&lt;/p&gt;
&lt;h1 id=&#34;二基本分类&#34;&gt;二、基本分类
&lt;/h1&gt;&lt;img src=&#34;image-20240903143111694.png&#34; alt=&#34;image-20240903143111694&#34; style=&#34;zoom: 50%;&#34; /&gt;
&lt;p&gt;如上图所示，仅有正方形的数据点，无法学习真实的数据分布，通过引入大量的未标注的圆形点，可得知数据的真实情况，这便是半监督学习的基本原理。&lt;/p&gt;
&lt;p&gt;对于有标注的输入样本，因为有标签作为监督信号，所以很自然可以定义损失函数。对于未标注样本，需要自行构建监督信号。基本可分为两大类，一类是熵最小化约束，使模型趋向于得出高置信度的预测结果，另一类则是一致性约束，对输入或模型添加各种干扰，但模型依然具有稳定的输出。一致性约束是本文的重点，简单来说，就是通过各种奇技淫巧，使得模型对一个输入，具有不同的输出，然后让两个输出趋向一致，这就能构建出监督信号&lt;/p&gt;
&lt;h1 id=&#34;三一致性约束&#34;&gt;三、一致性约束
&lt;/h1&gt;&lt;h2 id=&#34;31-基本架构&#34;&gt;3.1 基本架构
&lt;/h2&gt;&lt;h3 id=&#34;311-mean-teacher&#34;&gt;3.1.1 mean teacher
&lt;/h3&gt;&lt;img src=&#34;image-20240903143141868.png&#34; alt=&#34;image-20240903143141868&#34; style=&#34;zoom: 67%;&#34; /&gt;
Mean Teacher模型是经典的半监督方法，学生模型通过EMA方法更新参数，然后约束教师模型和学生模型之间的一致性
&lt;h3 id=&#34;312-多个子模型&#34;&gt;3.1.2 多个子模型
&lt;/h3&gt;&lt;img src=&#34;image-20240903143200747.png&#34; alt=&#34;image-20240903143200747&#34; style=&#34;zoom:80%;&#34; /&gt;
直接构建两个独立的子模型，一个是卷积架构，另一个是transformer架构
&lt;h3 id=&#34;313-多个解码器&#34;&gt;3.1.3 多个解码器
&lt;/h3&gt;&lt;img src=&#34;image-20240903143221906.png&#34; alt=&#34;image-20240903143221906&#34; style=&#34;zoom: 67%;&#34; /&gt;
&lt;p&gt;多个解码器&lt;/p&gt;
&lt;h3 id=&#34;314-多尺度&#34;&gt;3.1.4 多尺度
&lt;/h3&gt;&lt;img src=&#34;image-20240903143240369.png&#34; alt=&#34;image-20240903143240369&#34; style=&#34;zoom: 67%;&#34; /&gt;
使得解码器的不同层次输出保持一致性
&lt;h2 id=&#34;32-技巧&#34;&gt;3.2 技巧
&lt;/h2&gt;&lt;p&gt;基本架构应该就是上述的模型，后面就是使用各种技巧去提高模型的性能&lt;/p&gt;
&lt;h3 id=&#34;321-不确定性&#34;&gt;3.2.1 不确定性
&lt;/h3&gt;&lt;img src=&#34;image-20240903143300654.png&#34; alt=&#34;image-20240903143300654&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;两输出中肯定存在某些区域更重要，通过不确定性对其加权&lt;/p&gt;
&lt;h3 id=&#34;322-损失函数&#34;&gt;3.2.2 损失函数
&lt;/h3&gt;&lt;img src=&#34;image-20240903143315908.png&#34; alt=&#34;image-20240903143315908&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;类似知识蒸馏的方法，对输出取softmax操作时，添加温度参数T，使得具有更多信息，再去保持一致性。&lt;/p&gt;
&lt;h1 id=&#34;参考文献&#34;&gt;参考文献
&lt;/h1&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://proceedings.neurips.cc/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[1] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results&lt;/a&gt;&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://proceedings.mlr.press/v172/luo22b/luo22b.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[2] Semi-Supervised Medical Image Segmentation via Cross Teaching between CNN and Transformer&lt;/a&gt;&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Ouali_Semi-Supervised_Semantic_Segmentation_With_Cross-Consistency_Training_CVPR_2020_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[3] Semi-Supervised Semantic Segmentation with Cross-Consistency Training&lt;/a&gt;&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2012.07042.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[4] Efficient Semi-Supervised Gross Target Volume of Nasopharyngeal Carcinoma Segmentation via Uncertainty Rectified Pyramid Consistency&lt;/a&gt;&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1907.07034.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[5] Uncertainty-aware Self-ensembling Model for Semi-supervised 3D Left Atrium Segmentation&lt;/a&gt;&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2109.09960.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[6] Mutual Consistency Learning for Semi-supervised Medical Image Segmentation&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>改造nnUnet，嵌入第三方网络</title>
        <link>https://wakingupdelta.github.io/p/%E6%94%B9%E9%80%A0nnunet%E5%B5%8C%E5%85%A5%E7%AC%AC%E4%B8%89%E6%96%B9%E7%BD%91%E7%BB%9C/</link>
        <pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate>
        
        <guid>https://wakingupdelta.github.io/p/%E6%94%B9%E9%80%A0nnunet%E5%B5%8C%E5%85%A5%E7%AC%AC%E4%B8%89%E6%96%B9%E7%BD%91%E7%BB%9C/</guid>
        <description>&lt;h1 id=&#34;引言&#34;&gt;引言
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;nnUnet&lt;/strong&gt; 是有监督的医学图像分割绕不开的话题，其卓越的性能和简易的方法，为相关研究者提供了一项强有力的工具。然而，由于高度封装性，在原先代码中嵌入自定义网络进行训练，并不是十分方便，本文旨在分享一点在使用 &lt;strong&gt;nnUnet&lt;/strong&gt; 训练自定义网络过程中的一点经验，可能存在纰漏，欢迎在讨论区交流！&lt;/p&gt;
&lt;h1 id=&#34;一配置环境&#34;&gt;一、配置环境
&lt;/h1&gt;&lt;h2 id=&#34;11-硬件需求&#34;&gt;1.1 硬件需求
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;nnUnet&lt;/strong&gt; 的建议环境是Linux，若使用Windows，需修改路径相关代码（斜杠和反斜杠的替换），很麻烦（不推荐）。博主是在Ubuntu环境中使用Pycharm进行 &lt;strong&gt;nnUnet&lt;/strong&gt; 的学习&lt;/p&gt;
&lt;h2 id=&#34;12-软件需求&#34;&gt;1.2 软件需求
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;nnUnet&lt;/strong&gt; 官方推荐的使用方法是在命令行，但这不方便初学者学习。为了使用Pycharm的调试功能，需修改两个文件的代码 &lt;em&gt;&lt;strong&gt;nnunetv2/paths.py&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;nnunetv2/run/run_training.py&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;121-数据集路径&#34;&gt;1.2.1 数据集路径
&lt;/h3&gt;&lt;p&gt;位于 &lt;em&gt;&lt;strong&gt;nnunetv2/paths.py&lt;/strong&gt;&lt;/em&gt; 文件中，将三个变量路径修改为自己的路径。&lt;strong&gt;custom_&lt;/strong&gt; 是博主自己定义的文件，大家可以随意实现&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;custom_&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;custom_config&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;base&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;custom_config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;base&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;preprocessing_output_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;custom_config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;preprocessing_output_dir&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;network_training_output_dir_base&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;custom_config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;network_training_output_dir_base&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;122-程序入口&#34;&gt;1.2.2 程序入口
&lt;/h3&gt;&lt;p&gt;位于 &lt;em&gt;&lt;strong&gt;nnunetv2/run/run_training.py&lt;/strong&gt;&lt;/em&gt; 文件中，这里nnUnet训练代码的入口。由于不是命令行调用方式，需要将parser.add_argument的传入参数修改，添加 “-” 并设置 default 值。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;argparse&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_argument&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;-network&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2d&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_argument&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;-network_trainer&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;nnUNetTrainerV2&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_argument&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;-task&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;666&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;help&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;can be task name or task id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;parser&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_argument&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;-fold&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;help&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;0, 1, ..., 5 or &amp;#39;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;all&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;二构建网络&#34;&gt;二、构建网络
&lt;/h1&gt;&lt;h2 id=&#34;21-前置知识&#34;&gt;2.1 前置知识
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;nnUnet&lt;/strong&gt; 默认使用深监督，意味着自定义网络输出应为一个列表形式。然而，在网络推理时，我们只需要最高分辨率的输出，不需要多层次输出。在nnUnet官方实现中，使用 &lt;strong&gt;deep_supervision&lt;/strong&gt; 参数控制是否多层次输出。综上所述，自定义网络需要满足两个条件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持多层次输出&lt;/li&gt;
&lt;li&gt;使用变量deep_supervision控制输出类型&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;22-嵌入自定义网络&#34;&gt;2.2 嵌入自定义网络
&lt;/h2&gt;&lt;h3 id=&#34;221-包装网络&#34;&gt;2.2.1 包装网络
&lt;/h3&gt;&lt;p&gt;这里提供一种对已有网络包装的方法，仅供参考&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch.nn&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;nn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;custom_net&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;custom_net&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;deep_supervision&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 使用你自己的网络&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;deep_supervision&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;222-嵌入主框架&#34;&gt;2.2.2 嵌入主框架
&lt;/h3&gt;&lt;p&gt;将自定义网络嵌套进主框架。打开文件 &lt;strong&gt;nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;替换函数 &lt;strong&gt;build_network_architecture&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;build_network_architecture&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plans_manager&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PlansManager&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                   &lt;span class=&#34;n&#34;&gt;dataset_json&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                   &lt;span class=&#34;n&#34;&gt;configuration_manager&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ConfigurationManager&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                   &lt;span class=&#34;n&#34;&gt;num_input_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                   &lt;span class=&#34;n&#34;&gt;enable_deep_supervision&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;dynamic_network_architectures.initialization.weight_init&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;InitWeights_He&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;custom_net&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;InitWeights_He&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e-2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;参考资料&#34;&gt;参考资料
&lt;/h1&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.nature.com/articles/s41592-020-01008-z.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[1] nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation&lt;/a&gt;&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/MIC-DKFZ/nnUNet&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[2] nnUnet&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
